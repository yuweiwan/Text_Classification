{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from bayes_opt import BayesianOptimization\n",
    "from gensim import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500  # 表示样本表示最大的长度,表示降维之后的维度\n",
    "sentence_max_length = 1500  # 表示句子/样本在降维之前的维度\n",
    "Train_features3, Test_features3, Train_label3, Test_label3 = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练好的词嵌入向量和文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过models.KeyedVectors加载预训练好的embedding\n",
    "fast_embedding = models.FastText.load('/home/user10000411/notespace/Embedding/models/fast_model')\n",
    "w2v_embedding = models.Word2Vec.load('/home/user10000411/notespace/Embedding/models/w2v_model_50000')\n",
    "\n",
    "print(\"fast_embedding输出词表的个数{},w2v_embedding输出词表的个数{}\".format(\n",
    "    len(fast_embedding.wv.vocab.keys()), len(w2v_embedding.wv.vocab.keys())))\n",
    "\n",
    "print(\"取词向量成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取train_clean.tsv ，test_clean.tsv训练集和测试集文件\n",
    "# hint: 通过pandas中的read_csv读取数据集\n",
    "train = pd.read_csv('/home/user10000411/dataset/图书分类文本数据集/train_clean.csv', sep='\\t')\n",
    "test = pd.read_csv('/home/user10000411/dataset/图书分类文本数据集/test_clean.csv', sep='\\t')\n",
    "print(\"读取数据完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将df中的label映射为数字标签并保存到labelIndex列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName = train.label.unique()  # 全部label列表\n",
    "labelIndex = list(range(len(labelName)))  # 全部label标签\n",
    "labelNameToIndex = dict(zip(labelName, labelIndex))  # label的名字对应标签的字典\n",
    "labelIndexToName = dict(zip(labelIndex, labelName))  # label的标签对应名字的字典\n",
    "train[\"labelIndex\"] = train.label.map(labelNameToIndex)\n",
    "test[\"labelIndex\"] = test.label.map(labelNameToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[\"labelIndex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cut(query):\n",
    "    '''\n",
    "    函数说明：该函数用于对输入的语句（query）按照空格进行切分\n",
    "    '''\n",
    "    # 第一步：定义一个query_cut函数 将query按空格划分并返回，\n",
    "    query_list = query.split(' ')\n",
    "    return query_list\n",
    "\n",
    "# 第二步：然后train和test中的每一个样本都调用该函数，\n",
    "# 将划分好的样本分别存储到train[\"queryCut\"]和test[\"queryCut\"]中\n",
    "\n",
    "train[\"queryCut\"] = [query_cut(_) for _ in train.text]\n",
    "test[\"queryCut\"] = [query_cut(_) for _ in test.text]\n",
    "print(\"切分数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', \"r\") as f:\n",
    "    # 第一步：按行读取停用词文件\n",
    "    stopWords = []\n",
    "    for line in f:\n",
    "        stopWords.append(line.strip('\\n'))\n",
    "\n",
    "def rm_stop_word(wordList):\n",
    "    '''\n",
    "    函数说明：该函数用于去除输入样本中的存在的停用词\n",
    "    Return: 返回去除停用词之后的样本\n",
    "    '''\n",
    "    # 第二步：去除每个样本中的停用词并返回新的样本\n",
    "    forRemove = []\n",
    "    for word in wordList:\n",
    "        if word not in stopWords:\n",
    "            forRemove.append(word)            \n",
    "    \n",
    "    return forRemove\n",
    "    \n",
    "train[\"queryCutRMStopWord\"] = train[\"queryCut\"].apply(rm_stop_word)\n",
    "# dev[\"queryCutRMStopWord\"] = dev[\"text\"].apply(rm_stop_word)\n",
    "test[\"queryCutRMStopWord\"] = test[\"queryCut\"].apply(rm_stop_word)\n",
    "print(\"去除停用词\")\n",
    "print(type(train[\"queryCutRMStopWord\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cal(x):#对于矩阵的softmax, 输出与输入同形状\n",
    "    tmp = np.max(x,axis=1) # 得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "    x -= tmp.reshape((x.shape[0],1)) # 利用性质缩放元素\n",
    "    x = np.exp(x) # 计算所有值的指数\n",
    "    tmp = np.sum(x, axis = 1) # 每行求和        \n",
    "    x /= tmp.reshape((x.shape[0], 1)) # 求softmax\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(em, n): # 用于滑窗\n",
    "    wn = em.shape[0]\n",
    "    vn = em.shape[1]\n",
    "    temp = np.empty((wn-n+1,vn))\n",
    "   \n",
    "    for i in range(wn-n+1):\n",
    "        temp[i]=np.mean(em[i:i+n,:],axis=0)\n",
    "        \n",
    "    max_line = np.mean(temp, axis=0)\n",
    "    \n",
    "    return max_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cos(x,y):\n",
    "    v = np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_embedding_with_windows(embedding_matrix):\n",
    "    '''\n",
    "    函数说明：该函数用于获取在大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    参数说明：\n",
    "        - embedding_matrix：样本中所有词构成的词向量矩阵\n",
    "    return: result_list 返回拼接而成的一维词向量\n",
    "    '''\n",
    "    # 由于之前抽取的特征并没有考虑词与词之间交互对模型的影响，对于分类模型来说，贡献最大的可能是句子中的一部分， 如短语、词组等等。 \n",
    "    # 用大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    length = embedding_matrix.shape[0]\n",
    "    if length>3:\n",
    "        maxline2 = store(embedding_matrix,2)\n",
    "        maxline3 = store(embedding_matrix,3)\n",
    "        maxline4 = store(embedding_matrix,4) \n",
    "    elif length==3:\n",
    "        maxline2 = store(embedding_matrix,2)\n",
    "        maxline3 = store(embedding_matrix,3)\n",
    "        maxline4 = maxline3\n",
    "    elif length==2:\n",
    "        maxline2 = store(embedding_matrix,2)\n",
    "        maxline3 = maxline2\n",
    "        maxline4 = maxline2\n",
    "    else:\n",
    "        rl = np.hstack((np.hstack((embedding_matrix, embedding_matrix)),\n",
    "                     embedding_matrix))\n",
    "        result_list = rl.reshape(900,)\n",
    "        return result_list\n",
    "        \n",
    "    result_list = np.hstack((np.hstack((maxline2, maxline3)),\n",
    "                     maxline4))\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Label_embedding(word_matrix, label_embedding):\n",
    "    '''\n",
    "    函数说明：获取到所有类别的 label embedding， 与输入的 word embedding 矩阵相乘， 对其结果进行 softmax 运算，\n",
    "            对 attention score 与输入的 word embedding 相乘的结果求平均或者取最大\n",
    "    return: (np.array 1D) the embedding by join label and word\n",
    "    '''\n",
    "    global train\n",
    "    label_emb = np.array([fast_embedding.wv.get_vector(s) for s in train['label'].unique()])\n",
    "    label_embedding = label_emb\n",
    "    # 第一步：基于consin相似度计算word embedding向量与label embedding之间的相似度\n",
    "    cosine = np.zeros((word_matrix.shape[0],label_embedding.shape[0]))\n",
    "    for i in range(word_matrix.shape[0]):\n",
    "        for j in range(label_embedding.shape[0]):\n",
    "            cosine[i,j]=cal_cos(word_matrix[i],label_embedding[j])\n",
    "    \n",
    "    # 第二步：通过softmax获取注意力分布\n",
    "    softmax = softmax_cal(cosine)\n",
    "    \n",
    "    # 第三步：将求得到的注意力分布与输入的word embedding相乘，并对结果进行最大化或求平均\n",
    "    mul = np.dot(word_matrix.T,softmax)\n",
    "    result_embedding = np.average(mul, axis=1)\n",
    "    \n",
    "    return result_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(query):\n",
    "    '''\n",
    "    函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "            第一：利用word-embedding的average pooling和max-pooling\n",
    "            第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "            第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "            另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "    参数说明：\n",
    "    - query:数据集中的每一个样本\n",
    "    return: 返回样本经过哦特征工程之后得到的词向量\n",
    "    '''\n",
    "    global max_length\n",
    "    arr = []\n",
    "    # 加载fast_embedding,w2v_embedding\n",
    "    global fast_embedding, w2v_embedding\n",
    "    fast_arr = np.array([fast_embedding.wv.get_vector(s)\n",
    "                         for s in query if s in fast_embedding.wv.vocab.keys()])\n",
    "    # 在fast_arr下滑动获取到的词向量\n",
    "    if len(fast_arr) > 0:\n",
    "        windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        result_attention_embedding = Find_Label_embedding(\n",
    "            fast_arr, fast_embedding)\n",
    "    else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "        # 这里300表示训练词嵌入设置的维度为300\n",
    "        windows_fastarr = np.zeros(300) \n",
    "        result_attention_embedding = np.zeros(300)\n",
    "\n",
    "    fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "    fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "    fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "    # 将多个embedding进行横向拼接\n",
    "    arr = np.hstack((np.hstack((fast_arr, windows_fastarr)),\n",
    "                     result_attention_embedding))\n",
    "    global sentence_max_length\n",
    "    # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "    result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "        arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimension_Reduction(Train, Test):\n",
    "    '''\n",
    "    函数说明：该函数通过PCA算法对样本进行降维。\n",
    "    参数说明：\n",
    "    - Train: 表示训练数据集\n",
    "    - Test: 表示测试数据集\n",
    "    Return: 返回降维之后的数据样本\n",
    "    '''\n",
    "    global max_length\n",
    "    pca = PCA(n_components=max_length)\n",
    "    pca_train = pca.fit_transform(Train)\n",
    "    pca_test = pca.fit_transform(Test)\n",
    "    return pca_train, pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Embedding():\n",
    "    '''\n",
    "    函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "    Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "    '''\n",
    "    print(\"获取样本表示中...\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    Train_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(train[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    Test_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(test[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    print(\"获取样本词表示完成\")\n",
    "    # 通过PCA对样本表示进行降维\n",
    "    # Train_features2, Test_features2 = Dimension_Reduction(Train=Train_features2, Test=Test_features2)\n",
    "    Train_label2 = train[\"labelIndex\"]\n",
    "    Test_label2 = test[\"labelIndex\"]\n",
    "\n",
    "    print(\"加载训练好的词向量\")\n",
    "    print(\"Train_features.shape =\", Train_features2.shape)\n",
    "    print(\"Test_features.shape =\", Test_features2.shape)\n",
    "    print(\"Train_label.shape =\", Train_label2.shape)\n",
    "    print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "    return Train_features2, Test_features2, Train_label2, Test_label2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, model_name):\n",
    "    '''\n",
    "    函数说明：直接输出训练集和测试在模型上的准确率\n",
    "    参数说明：\n",
    "        - Train_label: 真实的训练集标签（1D）\n",
    "        - Test_labelb: 真实的测试集标签（1D）\n",
    "        - Train_predict_label: 模型在训练集上的预测的标签(1D)\n",
    "        - Test_predict_label: 模型在测试集上的预测标签（1D）\n",
    "        - model_name: 表示训练好的模型\n",
    "    Return: None\n",
    "    '''\n",
    "    # 通过调用metrics.accuracy_score计算训练集和测试集上的准确率\n",
    "    acc_train = metrics.accuracy_score(Train_label,Train_predict_label)\n",
    "    #print(Search_Flag+model_name+'_'+'Train accuracy %s' % (acc_train))\n",
    "    print(model_name+'_'+'Train accuracy %s' % (acc_train))\n",
    "    # 测试集的准确率\n",
    "    acc_test = metrics.accuracy_score(Test_label,Test_predict_label)\n",
    "    #print(Search_Flag+model_name+'_'+'test accuracy %s' % (acc_test))\n",
    "    print(model_name+'_'+'Test accuracy %s' % (acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Train_model(Train_features, Test_features, Train_label, Test_label):\n",
    "    '''\n",
    "    函数说明：基于网格搜索优化的方法搜索模型最优参数，最后保存训练好的模型\n",
    "    参数说明：\n",
    "        - Train_features: 训练集特征数组（2D）\n",
    "        - Test_features: 测试集特征数组（2D）\n",
    "        - Train_label: 真实的训练集标签 (1D)\n",
    "        - Test_label: 真实的测试集标签（1D）\n",
    "    Return: None\n",
    "    '''\n",
    "    models = [\n",
    "        lgb.LGBMClassifier(max_depth=3, \n",
    "                           num_leaves=30,\n",
    "                           learning_rate=0.05, \n",
    "                           n_estimators=1000, \n",
    "                           min_child_weight=2, \n",
    "                           max_delta_step=0.2, \n",
    "                           subsample=0.8, #bagging_fraction\n",
    "                           bagging_freq= 8,\n",
    "                           colsample_bytree=0.8, # feature_fraction\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=10, \n",
    "                           scale_pos_weight=0.2,\n",
    "                           device=\"gpu\",\n",
    "                           max_bin=63)\n",
    "    ]\n",
    "    # 遍历模型\n",
    "    for model in models:\n",
    "        model_name = model.__class__.  __name__\n",
    "        #gsearch = GridSearchCV(model, param_grid=parameters, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "        #gsearch.fit(Train_features, Train_label)\n",
    "        model.fit(Train_features, Train_label)\n",
    "        # 输出最好的参数\n",
    "        #print(\"Best parameters set found on development set:{}\".format(\n",
    "            #gsearch.best_params_))\n",
    "        #Test_predict_label = gsearch.predict(Test_features)\n",
    "        #Train_predict_label = gsearch.predict(Train_features)\n",
    "        #Predict(Train_label, Test_label,\n",
    "                #Train_predict_label, Test_predict_label, model_name)\n",
    "        Test_predict_label = model.predict(Test_features)\n",
    "        Train_predict_label = model.predict(Train_features)\n",
    "        # print(Test_predict_label)\n",
    "        Predict(Train_label, Test_label,\n",
    "                Train_predict_label, Test_predict_label, model_name)\n",
    "\n",
    "\n",
    "    # 保存训练好的模型\n",
    "    joblib.dump(model, 'LightGBM.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数,先求训练集和测试集的词向量，然后根据Grid搜索来找到最佳参数的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_features, Test_features, Train_label, Test_label = Find_Embedding()\n",
    "Grid_Train_model(Train_features, Test_features,Train_label, Test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：实际运行发现并不用降维，降维没有增速太多但是准确率大幅下降"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
